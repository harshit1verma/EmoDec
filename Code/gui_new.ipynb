{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import tkinter as tk\n",
    "from tkinter import *\n",
    "from PIL import Image, ImageTk\n",
    "from keras.models import load_model\n",
    "import signal\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading saved the model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model = load_model('emotion_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating dictionary for the emotions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining path for our emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_dict={0:\"./emojis/angry.png\", 1:\"./emojis/disgusted.png\", 2:\"./emojis/fearful.png\", 3:\"./emojis/happy.png\", 4:\"./emojis/neutral.png\", 5:\"./emojis/sad.png\", 6:\"./emojis/surpriced.png\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global variables which will control the GUI and its output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time after which the camera will click a picture that\n",
    "# will be predicted by the model. It is in milliseconds\n",
    "#camera_capturing_interval = 600\n",
    "\n",
    "# this is interval in seconds at which camera capture\n",
    "# image\n",
    "camera_capturing_interval_seconds = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining function to show subject "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_live_video():\n",
    "    \"\"\"This function responsibility is to capture video frame and predict\n",
    "    emotion of person update the cam window by making recangle around\n",
    "    face and show the emotion in text above the rectangle.-=\n",
    "    This function calls itself to capture the image after some period.\n",
    "    \"\"\"\n",
    "    # this is the predicted emotion for each captured frame\n",
    "    # make angry as defaut so that on first frame error will not come \n",
    "    pridicted_emotion = 0 \n",
    "\n",
    "    # this is the current frame we are predicting. \n",
    "    # we can consider it as the number of picture some camera took\n",
    "    # when you go on a vacation. This number will tell how many\n",
    "    # predictions were there on how many captured frames\n",
    "    frame_number = 0\n",
    "\n",
    "    # Live camera. 0 = live, video_path string = play video       \n",
    "    camera = cv2.VideoCapture(0)                                 \n",
    "    if not camera.isOpened():                             \n",
    "        print(\"cant open the camera1\")\n",
    "\n",
    "    # length = int(camera.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # print(length, frame_number)\n",
    "\n",
    "    frame_number += 1\n",
    "    # if frame_number >= length:\n",
    "    #     exit()\n",
    "\n",
    "    # 0. CV_CAP_PROP_POS_MSEC Current position of the video file in milliseconds.\n",
    "    ##USED## 1. CV_CAP_PROP_POS_FRAMES 0-based index of the frame to be decoded/captured next.\n",
    "    # 2. CV_CAP_PROP_POS_AVI_RATIO Relative position of the video file\n",
    "    # 3. CV_CAP_PROP_FRAME_WIDTH Width of the frames in the video stream.\n",
    "    # 4. CV_CAP_PROP_FRAME_HEIGHT Height of the frames in the video stream.\n",
    "    # 5. CV_CAP_PROP_FPS Frame rate.\n",
    "    # 6. CV_CAP_PROP_FOURCC 4-character code of codec.\n",
    "    # 7. CV_CAP_PROP_FRAME_COUNT Number of frames in the video file.\n",
    "    # 8. CV_CAP_PROP_FORMAT Format of the Mat objects returned by retrieve() .\n",
    "    # 9. CV_CAP_PROP_MODE Backend-specific value indicating the current capture mode.\n",
    "    # 10. CV_CAP_PROP_BRIGHTNESS Brightness of the image (only for cameras).\n",
    "    # 11. CV_CAP_PROP_CONTRAST Contrast of the image (only for cameras).\n",
    "    # 12. CV_CAP_PROP_SATURATION Saturation of the image (only for cameras).\n",
    "    # 13. CV_CAP_PROP_HUE Hue of the image (only for cameras).\n",
    "    # 14. CV_CAP_PROP_GAIN Gain of the image (only for cameras).\n",
    "    # 15. CV_CAP_PROP_EXPOSURE Exposure (only for cameras).\n",
    "    # 16. CV_CAP_PROP_CONVERT_RGB Boolean flags indicating whether images should be converted to RGB.\n",
    "    # 17. CV_CAP_PROP_WHITE_BALANCE Currently unsupported\n",
    "    # 18. CV_CAP_PROP_RECTIFICATION Rectification flag for stereo cameras (note: only supported by DC1394 v 2.x backend currently)\n",
    "    \n",
    "    # to set the camera frame setting accordingly. Here we need zero based index frame. ie frame values start from 0 rather than 1.\n",
    "    camera.set(1, frame_number)\n",
    "\n",
    "    is_frame_read_success, video_frame = camera.read()\n",
    "\n",
    "    # Never resize. change the position of the frame. Else our video got laggy\n",
    "    # video_frame = cv2.resize(video_frame, (600, 500))\n",
    "\n",
    "    # A blue bounding box around face\n",
    "    # to understand CascadeClassifier https://docs.opencv.org/3.1.0/d7/d8b/tutorial_py_face_detection.html#gsc.tab=0\n",
    "    face_bounding_box = cv2.CascadeClassifier('C:/3-Programming/EmoDec/.emodec_venv/Lib/site-packages/cv2/data/haarcascade_frontalface_default.xml')\n",
    "    \n",
    "    # this issue happens due to camera api used cv2. it is best performed on inbuild camera of laptop\n",
    "    # if external camera is used this error occurs\n",
    "    try:\n",
    "        # converting colored frame to grayscale as training data was in grayscale\n",
    "        gray_frame = cv2.cvtColor(video_frame, cv2.COLOR_BGR2GRAY)\n",
    "    except cv2.error:\n",
    "        print('Error in capturing image from camera')\n",
    "        return\n",
    "\n",
    "    # Check paramaters of detectMultiScale here https://stackoverflow.com/questions/36218385/parameters-of-detectmultiscale-in-opencv-using-python\n",
    "    num_faces = face_bounding_box.detectMultiScale(gray_frame, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    # Each face will give you the coordinate of point, height and width so that a rectangle can be formed\n",
    "    for (x, y, w, h) in num_faces:\n",
    "\n",
    "        # TODO: how rectangle is created in cv2 documentation\n",
    "        cv2.rectangle(video_frame, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n",
    "\n",
    "        # cropping image of face from full frame of video according to trainning data of model\n",
    "        roi_gray_frame = gray_frame[y:y + h, x:x + w]\n",
    "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)\n",
    "        \n",
    "        # predicting cropped face image\n",
    "        prediction = emotion_model.predict(cropped_img)\n",
    "        \n",
    "        # give index of maximum probabiliy of emotion \n",
    "        maxindex = int(np.argmax(prediction))\n",
    "\n",
    "        # print(emotion_dict[maxindex])\n",
    "\n",
    "        # Text of emotion above the face box\n",
    "        cv2.putText(video_frame, emotion_dict[maxindex], (x+20, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        pridicted_emotion = maxindex\n",
    "        \n",
    "    if is_frame_read_success is None:\n",
    "        print (\"Major error!\")\n",
    "    elif is_frame_read_success:\n",
    "        # here we are creating a copy of captured video frame for predicting emotion.\n",
    "        captured_frame = video_frame.copy()\n",
    "\n",
    "        # converting captured frame to RGB(red, green, blue) real color\n",
    "        # BGR (blue, green, red)\n",
    "        pic = cv2.cvtColor(captured_frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # converting array of pixels into actual image\n",
    "        img = Image.fromarray(pic)\n",
    "\n",
    "        # now converting that image object in binary into a proper image object used PIL\n",
    "        imgtk = ImageTk.PhotoImage(image=img)\n",
    "\n",
    "        # cam_window is used to insert the captured image by video camera then insert that captured\n",
    "        # frame to cam_window frame. which looks like camera is recording the reaction of the person\n",
    "        cam_window.imgtk = imgtk\n",
    "\n",
    "        # her econfigure is needed to updated the image at that lable or the window that is created\n",
    "        cam_window.configure(image=imgtk)\n",
    "\n",
    "        # then update the root as we need to refresh root window to see the changes in cam window.\n",
    "        # root.update()\n",
    "\n",
    "        # then call the itself like a recursive function so that next frame should be caputured after \n",
    "        # required milliseconds and whole process of capturing frame and predicting emotion\n",
    "        # can repeat itself.\n",
    "        # cam_window.after(camera_capturing_interval, capture_live_video)\n",
    "\n",
    "        # this function is called to update the emoji avatar on prediction\n",
    "        show_avatar(pridicted_emotion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining function to show avatar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_avatar(pridicted_emotion):\n",
    "    \"\"\"This function is used to show avatar with emotion in the UI on the right.\n",
    "    Main intention to show avatar is to show the replica of human emotion.\n",
    "    \"\"\"\n",
    "    # this is the avatar image of the emotion read by cv2\n",
    "    avatar_image = cv2.imread(emoji_dict[pridicted_emotion])\n",
    "\n",
    "    # here we changing the avatar image from BGR to RGB\n",
    "    pic = cv2.cvtColor(avatar_image ,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Then converting this array of pixels to binary image object\n",
    "    image = Image.fromarray(pic)\n",
    "    \n",
    "    # now creating image object which can be manupulated. this \n",
    "    # image object is created through PIL library\n",
    "    imgtk=ImageTk.PhotoImage(image=image)\n",
    "\n",
    "    # now configuring emoji avatar window with the image object which is to be\n",
    "    # show as human replica of emotion\n",
    "    emoji_avatar.imgtk=imgtk\n",
    "    \n",
    "    # first the text is confugeres shown above the avatar\n",
    "    emoji_label.configure(text=emotion_dict[pridicted_emotion],font=('arial',45,'bold'))\n",
    "    \n",
    "    # then the emoji avatar is configured to show the image of the avatar on emoji avatar\n",
    "    # window\n",
    "    emoji_avatar.configure(image=imgtk)\n",
    "\n",
    "    # updating root window is important else you will not see the updated avatar in\n",
    "    # in avatar window\n",
    "    # root.update()\n",
    "\n",
    "    # show emoji avatar just when the camera image is captured and predicted.\n",
    "    # emoji_avatar.after(camera_capturing_interval, show_avatar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signal_handler(signum, frame):\n",
    "    \"\"\"This function will handle what happens\n",
    "    when CTRL + C is typed on the keyboard\n",
    "    \"\"\"\n",
    "    global terminate\n",
    "\n",
    "    terminate = True\n",
    "    root.destroy()\n",
    "    sys.exit()\n",
    "\n",
    "def terminate_script():\n",
    "    \"\"\"This will execute when exit button is clicked\n",
    "    on tkinter interface, then it will exit the \n",
    "    programme and destory tk inter ui.\n",
    "    \"\"\"\n",
    "    global terminate\n",
    "\n",
    "    terminate = True\n",
    "    root.destroy()\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting GUI window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    " # this variable will break the main loop of capturing image\n",
    "terminate = False\n",
    "\n",
    "# this capture the exit signal that is CTRL + C\n",
    "signal.signal(signal.SIGINT, signal_handler)\n",
    "\n",
    "# root is the whole window that tkinter opened as ui\n",
    "root = tk.Tk()   \n",
    "\n",
    "# Window at which the camera will record your videos\n",
    "cam_window = tk.Label(master=root, padx=50, bd=10)\n",
    "cam_window.pack(side=LEFT)\n",
    "cam_window.place(x=50,y=50)\n",
    "\n",
    "# avatar of emoji\n",
    "emoji_avatar = tk.Label(master=root, bd=10)\n",
    "emoji_avatar.pack(side=RIGHT)\n",
    "emoji_avatar.place(x=900,y=150)\n",
    "\n",
    "# Emoji avatar label window\n",
    "emoji_label = tk.Label(master=root, bd=10, fg=\"#CDCDCD\", bg='black')\n",
    "emoji_label.pack()\n",
    "emoji_label.place(x=960, y=50)\n",
    "\n",
    "root.title(\"EmoDec\")            \n",
    "root.geometry(\"1400x700+100+10\") \n",
    "root['bg']='grey'\n",
    "\n",
    "# Quit button\n",
    "exitbutton = Button(root, text='Quit', fg=\"red\", command=terminate_script, font=('arial', 25, 'bold')).pack(side = BOTTOM)\n",
    "# threading.Thread(target=capture_live_video).start()\n",
    "# threading.Thread(target=show_avatar).start()\n",
    "# root.mainloop()\n",
    "while not terminate:\n",
    "    capture_live_video()\n",
    "    root.update_idletasks()\n",
    "    root.update()\n",
    "    time.sleep(camera_capturing_interval_seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "94ad695ac05f107f4ad12712c867fa3e4427199706608624ba769dffc8b7d7cc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('.emodec_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
