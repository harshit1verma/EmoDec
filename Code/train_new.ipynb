{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.models import save_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the training and validation generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# Assining the directory of our data \n",
    "train_dir = 'data/train'     \n",
    "val_dir = 'data/test'\n",
    "\n",
    "# Generating greyscale image data in terms of 0-1 of pixels \n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Reading our generated data from ImageDataGenerator\n",
    "# flow_from_directory when we have sub folders in directory \n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # directory location\n",
    "        train_dir,\n",
    "        # Size of images to be selected \n",
    "        target_size=(48,48),\n",
    "        # Number of images to be selected \n",
    "        batch_size=64,\n",
    "        color_mode=\"grayscale\",\n",
    "        # Type of class to predict \n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        val_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=64,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the convolution network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"We have two options to create neural network in keras sequential and functional \n",
    "where sequential allows us to create model layer by layer in a sequence with only single \n",
    "input and single output.\n",
    "\"\"\"\n",
    "emotion_model = Sequential()\n",
    "\n",
    "# Conv2D creates a convolution kernal which acts as a filter to produce tensor of outputs.\n",
    "# Starting with 32 filters, matrix size of 3x3,\n",
    "# Using relu(Rectified linear unit) as activation function which decides if neuron should activate or not.\n",
    "# MaxPooling helps in downsampling our data.\n",
    "emotion_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Conv2D(256, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# Droupout is used to prevent overfiting \n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "# Flatten is used to convert 2d array into 1d array\n",
    "emotion_model.add(Flatten())\n",
    "# Dense layer is simple layer of neurons which is used to \n",
    "# receives input from all the neurons of previous layer\n",
    "emotion_model.add(Dense(1024, activation='relu'))\n",
    "emotion_model.add(Dropout(0.5))\n",
    "# Softmax function is used for classification\n",
    "emotion_model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling and Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\3-Programming\\EmoDec\\.emodec_venv\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "C:\\Users\\Asus\\AppData\\Local\\Temp/ipykernel_5412/2449506319.py:8: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  emotion_model_info = emotion_model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "448/448 [==============================] - 53s 116ms/step - loss: 1.8280 - accuracy: 0.2476 - val_loss: 1.8294 - val_accuracy: 0.2478\n",
      "Epoch 2/50\n",
      "448/448 [==============================] - 42s 94ms/step - loss: 1.7976 - accuracy: 0.2526 - val_loss: 1.7808 - val_accuracy: 0.2693\n",
      "Epoch 3/50\n",
      "448/448 [==============================] - 40s 90ms/step - loss: 1.7268 - accuracy: 0.3014 - val_loss: 1.6224 - val_accuracy: 0.3763\n",
      "Epoch 4/50\n",
      "448/448 [==============================] - 42s 93ms/step - loss: 1.6048 - accuracy: 0.3737 - val_loss: 1.5344 - val_accuracy: 0.4121\n",
      "Epoch 5/50\n",
      "448/448 [==============================] - 40s 88ms/step - loss: 1.5375 - accuracy: 0.4021 - val_loss: 1.4901 - val_accuracy: 0.4174\n",
      "Epoch 6/50\n",
      "448/448 [==============================] - 40s 90ms/step - loss: 1.4881 - accuracy: 0.4242 - val_loss: 1.4352 - val_accuracy: 0.4528\n",
      "Epoch 7/50\n",
      "448/448 [==============================] - 42s 94ms/step - loss: 1.4446 - accuracy: 0.4437 - val_loss: 1.4016 - val_accuracy: 0.4637\n",
      "Epoch 8/50\n",
      "448/448 [==============================] - 40s 88ms/step - loss: 1.4052 - accuracy: 0.4619 - val_loss: 1.3619 - val_accuracy: 0.4813\n",
      "Epoch 9/50\n",
      "448/448 [==============================] - 40s 90ms/step - loss: 1.3676 - accuracy: 0.4755 - val_loss: 1.3269 - val_accuracy: 0.4950\n",
      "Epoch 10/50\n",
      "448/448 [==============================] - 41s 91ms/step - loss: 1.3395 - accuracy: 0.4918 - val_loss: 1.3143 - val_accuracy: 0.4976\n",
      "Epoch 11/50\n",
      "448/448 [==============================] - 40s 90ms/step - loss: 1.3091 - accuracy: 0.5040 - val_loss: 1.2756 - val_accuracy: 0.5142\n",
      "Epoch 12/50\n",
      "448/448 [==============================] - 37s 82ms/step - loss: 1.2859 - accuracy: 0.5131 - val_loss: 1.2641 - val_accuracy: 0.5205\n",
      "Epoch 13/50\n",
      "448/448 [==============================] - 39s 88ms/step - loss: 1.2634 - accuracy: 0.5216 - val_loss: 1.2468 - val_accuracy: 0.5265\n",
      "Epoch 14/50\n",
      "448/448 [==============================] - 40s 89ms/step - loss: 1.2422 - accuracy: 0.5353 - val_loss: 1.2277 - val_accuracy: 0.5329\n",
      "Epoch 15/50\n",
      "448/448 [==============================] - 42s 93ms/step - loss: 1.2207 - accuracy: 0.5392 - val_loss: 1.2194 - val_accuracy: 0.5318\n",
      "Epoch 16/50\n",
      "448/448 [==============================] - 40s 89ms/step - loss: 1.2030 - accuracy: 0.5492 - val_loss: 1.1977 - val_accuracy: 0.5424\n",
      "Epoch 17/50\n",
      "448/448 [==============================] - 41s 92ms/step - loss: 1.1821 - accuracy: 0.5535 - val_loss: 1.1880 - val_accuracy: 0.5519\n",
      "Epoch 18/50\n",
      "448/448 [==============================] - 40s 90ms/step - loss: 1.1658 - accuracy: 0.5626 - val_loss: 1.1859 - val_accuracy: 0.5501\n",
      "Epoch 19/50\n",
      "448/448 [==============================] - 40s 90ms/step - loss: 1.1471 - accuracy: 0.5682 - val_loss: 1.1676 - val_accuracy: 0.5512\n",
      "Epoch 20/50\n",
      "448/448 [==============================] - 40s 90ms/step - loss: 1.1329 - accuracy: 0.5739 - val_loss: 1.1546 - val_accuracy: 0.5604\n",
      "Epoch 21/50\n",
      "448/448 [==============================] - 40s 88ms/step - loss: 1.1121 - accuracy: 0.5821 - val_loss: 1.1485 - val_accuracy: 0.5665\n",
      "Epoch 22/50\n",
      "448/448 [==============================] - 40s 90ms/step - loss: 1.0980 - accuracy: 0.5860 - val_loss: 1.1446 - val_accuracy: 0.5677\n",
      "Epoch 23/50\n",
      "448/448 [==============================] - 39s 87ms/step - loss: 1.0820 - accuracy: 0.5970 - val_loss: 1.1416 - val_accuracy: 0.5685\n",
      "Epoch 24/50\n",
      "448/448 [==============================] - 40s 89ms/step - loss: 1.0646 - accuracy: 0.6030 - val_loss: 1.1336 - val_accuracy: 0.5640\n",
      "Epoch 25/50\n",
      "448/448 [==============================] - 40s 89ms/step - loss: 1.0561 - accuracy: 0.6050 - val_loss: 1.1304 - val_accuracy: 0.5684\n",
      "Epoch 26/50\n",
      "448/448 [==============================] - 40s 89ms/step - loss: 1.0366 - accuracy: 0.6149 - val_loss: 1.1271 - val_accuracy: 0.5716\n",
      "Epoch 27/50\n",
      "448/448 [==============================] - 41s 92ms/step - loss: 1.0222 - accuracy: 0.6181 - val_loss: 1.1247 - val_accuracy: 0.5699\n",
      "Epoch 28/50\n",
      "448/448 [==============================] - 40s 89ms/step - loss: 1.0105 - accuracy: 0.6223 - val_loss: 1.1263 - val_accuracy: 0.5717\n",
      "Epoch 29/50\n",
      "448/448 [==============================] - 40s 89ms/step - loss: 0.9918 - accuracy: 0.6342 - val_loss: 1.1228 - val_accuracy: 0.5734\n",
      "Epoch 30/50\n",
      "448/448 [==============================] - 40s 89ms/step - loss: 0.9772 - accuracy: 0.6353 - val_loss: 1.1122 - val_accuracy: 0.5841\n",
      "Epoch 31/50\n",
      "448/448 [==============================] - 40s 89ms/step - loss: 0.9629 - accuracy: 0.6437 - val_loss: 1.1146 - val_accuracy: 0.5819\n",
      "Epoch 32/50\n",
      "448/448 [==============================] - 40s 89ms/step - loss: 0.9497 - accuracy: 0.6472 - val_loss: 1.1037 - val_accuracy: 0.5801\n",
      "Epoch 33/50\n",
      "448/448 [==============================] - 40s 90ms/step - loss: 0.9324 - accuracy: 0.6554 - val_loss: 1.1233 - val_accuracy: 0.5845\n",
      "Epoch 34/50\n",
      "448/448 [==============================] - 40s 89ms/step - loss: 0.9218 - accuracy: 0.6599 - val_loss: 1.1027 - val_accuracy: 0.5894\n",
      "Epoch 35/50\n",
      "448/448 [==============================] - 40s 89ms/step - loss: 0.9084 - accuracy: 0.6613 - val_loss: 1.0992 - val_accuracy: 0.5954\n",
      "Epoch 36/50\n",
      "448/448 [==============================] - 40s 89ms/step - loss: 0.8958 - accuracy: 0.6701 - val_loss: 1.1282 - val_accuracy: 0.5823\n",
      "Epoch 37/50\n",
      "448/448 [==============================] - 40s 89ms/step - loss: 0.8758 - accuracy: 0.6769 - val_loss: 1.0997 - val_accuracy: 0.5961\n",
      "Epoch 38/50\n",
      "448/448 [==============================] - 40s 89ms/step - loss: 0.8631 - accuracy: 0.6814 - val_loss: 1.0956 - val_accuracy: 0.5957\n",
      "Epoch 39/50\n",
      "448/448 [==============================] - 40s 89ms/step - loss: 0.8422 - accuracy: 0.6900 - val_loss: 1.1011 - val_accuracy: 0.5963\n",
      "Epoch 40/50\n",
      "448/448 [==============================] - 40s 89ms/step - loss: 0.8329 - accuracy: 0.6901 - val_loss: 1.1054 - val_accuracy: 0.5965\n",
      "Epoch 41/50\n",
      "448/448 [==============================] - 40s 89ms/step - loss: 0.8210 - accuracy: 0.6989 - val_loss: 1.1110 - val_accuracy: 0.5965\n",
      "Epoch 42/50\n",
      "448/448 [==============================] - 38s 84ms/step - loss: 0.8036 - accuracy: 0.7030 - val_loss: 1.1537 - val_accuracy: 0.5883\n",
      "Epoch 43/50\n",
      "448/448 [==============================] - 35s 79ms/step - loss: 0.7869 - accuracy: 0.7128 - val_loss: 1.1338 - val_accuracy: 0.5995\n",
      "Epoch 44/50\n",
      "448/448 [==============================] - 35s 79ms/step - loss: 0.7736 - accuracy: 0.7153 - val_loss: 1.1483 - val_accuracy: 0.5972\n",
      "Epoch 45/50\n",
      "448/448 [==============================] - 35s 78ms/step - loss: 0.7578 - accuracy: 0.7237 - val_loss: 1.1672 - val_accuracy: 0.5943\n",
      "Epoch 46/50\n",
      "448/448 [==============================] - 35s 79ms/step - loss: 0.7443 - accuracy: 0.7277 - val_loss: 1.1373 - val_accuracy: 0.5968\n",
      "Epoch 47/50\n",
      "448/448 [==============================] - 35s 78ms/step - loss: 0.7249 - accuracy: 0.7354 - val_loss: 1.1328 - val_accuracy: 0.5953\n",
      "Epoch 48/50\n",
      "448/448 [==============================] - 35s 78ms/step - loss: 0.7236 - accuracy: 0.7339 - val_loss: 1.1443 - val_accuracy: 0.5967\n",
      "Epoch 49/50\n",
      "448/448 [==============================] - 35s 78ms/step - loss: 0.7058 - accuracy: 0.7410 - val_loss: 1.1477 - val_accuracy: 0.6038\n",
      "Epoch 50/50\n",
      "448/448 [==============================] - 38s 86ms/step - loss: 0.6921 - accuracy: 0.7441 - val_loss: 1.1674 - val_accuracy: 0.5982\n"
     ]
    }
   ],
   "source": [
    "# Loss function is used to find error or deviation in the learning process.\n",
    "# Optimizer assigns the input weights by comparing the prediction and the loss function.\n",
    "# Metrics is used to evaluate the performance of our model.\n",
    "# lr is learning rate and decay is learning rate decay over each update.\n",
    "emotion_model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.0001, decay=1e-6),metrics=['accuracy'])\n",
    "\n",
    "# Now we train using .fit_generator() method.\n",
    "emotion_model_info = emotion_model.fit_generator(\n",
    "        train_generator,\n",
    "        # Total number of training examples present in a single batch.\n",
    "        steps_per_epoch=28709 // 64,\n",
    "        # Epoch is when an ENTIRE dataset is passed forward and backward \n",
    "        # through the neural network only ONCE.\n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=7178 // 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(emotion_model,'emotion_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "94ad695ac05f107f4ad12712c867fa3e4427199706608624ba769dffc8b7d7cc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('.emodec_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
